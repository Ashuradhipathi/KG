{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q llama-index google-generativeai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.llms import Gemini\n",
    "\n",
    "# resp = Gemini().complete(\"Write a poem about a magic backpack\")\n",
    "# print(resp)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: docx2txt in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.8)\n"
     ]
    }
   ],
   "source": [
    "!pip install docx2txt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import (\n",
    "#     SimpleDirectoryReader,\n",
    "#     ServiceContext,\n",
    "#     KnowledgeGraphIndex,\n",
    "# )\n",
    "# from llama_index.graph_stores import SimpleGraphStore\n",
    "\n",
    "# from llama_index.llms import Gemini\n",
    "# from IPython.display import Markdown, display"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "# llm = Gemini(GOOGLE_API_KEY)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.vector_stores.google.generativeai import (\n",
    "#     google_service_context,\n",
    "# )\n",
    "# from llama_index import ServiceContext\n",
    "# service_context=ServiceContext.from_defaults(\n",
    "#         # Use Gemini to rerank the passages.\n",
    "#         llm=llm,\n",
    "#         # Reranker does not need embedding. It processes the original text.\n",
    "#         embed_model=None,\n",
    "#     )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index.storage.storage_context import StorageContext\n",
    "\n",
    "# graph_store = SimpleGraphStore()\n",
    "# storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "# # NOTE: can take a while!\n",
    "# index = KnowledgeGraphIndex.from_documents(\n",
    "#     documents,\n",
    "#     max_triplets_per_chunk=2,\n",
    "#     storage_context=storage_context,\n",
    "#     service_context=google_service_context,\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Requirement already satisfied: pyvis in /usr/local/python/3.10.13/lib/python3.10/site-packages (0.3.2)\n",
      "Requirement already satisfied: ipython>=5.3.0 in /home/codespace/.local/lib/python3.10/site-packages (from pyvis) (8.20.0)\n",
      "Requirement already satisfied: jinja2>=2.9.6 in /home/codespace/.local/lib/python3.10/site-packages (from pyvis) (3.1.2)\n",
      "Requirement already satisfied: jsonpickle>=1.4.1 in /usr/local/python/3.10.13/lib/python3.10/site-packages (from pyvis) (3.0.2)\n",
      "Requirement already satisfied: networkx>=1.11 in /home/codespace/.local/lib/python3.10/site-packages (from pyvis) (3.2.1)\n",
      "Requirement already satisfied: decorator in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (5.1.1)\n",
      "Requirement already satisfied: jedi>=0.16 in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.19.1)\n",
      "Requirement already satisfied: matplotlib-inline in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.1.6)\n",
      "Requirement already satisfied: prompt-toolkit<3.1.0,>=3.0.41 in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (3.0.43)\n",
      "Requirement already satisfied: pygments>=2.4.0 in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (2.17.2)\n",
      "Requirement already satisfied: stack-data in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (0.6.3)\n",
      "Requirement already satisfied: traitlets>=5 in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (5.14.1)\n",
      "Requirement already satisfied: exceptiongroup in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (1.2.0)\n",
      "Requirement already satisfied: pexpect>4.3 in /home/codespace/.local/lib/python3.10/site-packages (from ipython>=5.3.0->pyvis) (4.9.0)\n",
      "Requirement already satisfied: MarkupSafe>=2.0 in /home/codespace/.local/lib/python3.10/site-packages (from jinja2>=2.9.6->pyvis) (2.1.3)\n",
      "Requirement already satisfied: parso<0.9.0,>=0.8.3 in /home/codespace/.local/lib/python3.10/site-packages (from jedi>=0.16->ipython>=5.3.0->pyvis) (0.8.3)\n",
      "Requirement already satisfied: ptyprocess>=0.5 in /home/codespace/.local/lib/python3.10/site-packages (from pexpect>4.3->ipython>=5.3.0->pyvis) (0.7.0)\n",
      "Requirement already satisfied: wcwidth in /home/codespace/.local/lib/python3.10/site-packages (from prompt-toolkit<3.1.0,>=3.0.41->ipython>=5.3.0->pyvis) (0.2.13)\n",
      "Requirement already satisfied: executing>=1.2.0 in /home/codespace/.local/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.0.1)\n",
      "Requirement already satisfied: asttokens>=2.1.0 in /home/codespace/.local/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (2.4.1)\n",
      "Requirement already satisfied: pure-eval in /home/codespace/.local/lib/python3.10/site-packages (from stack-data->ipython>=5.3.0->pyvis) (0.2.2)\n",
      "Requirement already satisfied: six>=1.12.0 in /home/codespace/.local/lib/python3.10/site-packages (from asttokens>=2.1.0->stack-data->ipython>=5.3.0->pyvis) (1.16.0)\n"
     ]
    }
   ],
   "source": [
    "!pip install pyvis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from pyvis.network import Network\n",
    "# import IPython.display as ipd\n",
    "\n",
    "# g = index.get_networkx_graph()\n",
    "# net = Network(notebook=True, directed=True)\n",
    "# net.from_nx(g)\n",
    "\n",
    "# # Display the graph directly in the notebook\n",
    "# net.show(\"temp.html\")\n",
    "# ipd.IFrame(src='temp.html', width=800, height=600)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "# from llama_index import ServiceContext\n",
    "# from llama_index.llms import Gemini\n",
    "# from llama_index.indices.knowledge_graph.base import KnowledgeGraphIndex\n",
    "# from llama_index.graph_stores.simple import SimpleGraphStore\n",
    "\n",
    "\n",
    "# DEFAULT_KG_TRIPLET_EXTRACT_PROMPT = \"Here\"\n",
    "\n",
    "# # Using the Gemini model as the LLM and assigning it with a hypothetical setup.\n",
    "# # Please replace \"models/gemini-pro\" with your Gemini model's path or details\n",
    "# gemini_llm = Gemini(model=\"models/gemini-pro\", temperature=0)\n",
    "\n",
    "# # Using Llama's built-in triplet template for knowledge graph\n",
    "# # In actual setup, you could define and use your own extraction prompt template\n",
    "# triplet_template = DEFAULT_KG_TRIPLET_EXTRACT_PROMPT\n",
    "\n",
    "# # Creating a ServiceContext\n",
    "# gemini_pro_context = ServiceContext(llm=gemini_llm, context_window=4096, num_output=256, chunk_size=512)\n",
    "\n",
    "# # Building a KnowledgeGraphIndex\n",
    "# graph_store = SimpleGraphStore()\n",
    "# kg_index = KnowledgeGraphIndex(\n",
    "#     kg_triple_extract_template=triplet_template,\n",
    "#     service_context=gemini_pro_context,\n",
    "#     graph_store=graph_store\n",
    "# )"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# query_engine = index.as_query_engine(\n",
    "#     include_text=False, response_mode=\"tree_summarize\"\n",
    "# )\n",
    "# response = query_engine.query(\n",
    "#     \"Explain about campus Nourish\",\n",
    "# )\n",
    "# display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "# # Import necessary libraries\n",
    "# from llama_index import ServiceContext\n",
    "# from llama_index.llms import Gemini\n",
    "\n",
    "# # Define your LLM using Gemini\n",
    "# llm = Gemini(model=\"models/gemini-pro\", temperature=0)\n",
    "\n",
    "# # Define the service context directly\n",
    "# service_context = ServiceContext(llm=llm, context_window=4096, num_output=256, chunk_size=512)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "metadata": {},
   "outputs": [],
   "source": [
    "from llama_index.graph_stores import SimpleGraphStore\n",
    "from llama_index.llms import Gemini\n",
    "from llama_index.embeddings import GeminiEmbedding\n",
    "from IPython.display import Markdown, display\n",
    "from llama_index import (\n",
    "    VectorStoreIndex,\n",
    "    SimpleDirectoryReader,\n",
    "    ServiceContext,\n",
    "    KnowledgeGraphIndex,\n",
    "    StorageContext,\n",
    ")\n",
    "# Load the documents\n",
    "documents = SimpleDirectoryReader(\"data\").load_data()\n",
    "\n",
    "# Create the Gemini model\n",
    "llm = Gemini(model=\"models/gemini-pro\", temperature=0)\n",
    "\n",
    "# Create the service context\n",
    "service_context = ServiceContext.from_defaults(llm=llm, chunk_size=512, embed_model=GeminiEmbedding,)\n",
    "\n",
    "# Create the graph store\n",
    "graph_store = SimpleGraphStore()\n",
    "\n",
    "# Create the storage context\n",
    "storage_context = StorageContext.from_defaults(graph_store=graph_store)\n",
    "\n",
    "# Create the Knowledge Graph Index\n",
    "index = KnowledgeGraphIndex.from_documents(\n",
    "    documents,\n",
    "    max_triplets_per_chunk=1,\n",
    "    storage_context=storage_context,\n",
    "    service_context=service_context,\n",
    ")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 58,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "example111.html\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"100%\"\n",
       "            height=\"600px\"\n",
       "            src=\"example111.html\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fb57dabe7d0>"
      ]
     },
     "execution_count": 58,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "## create graph\n",
    "from pyvis.network import Network\n",
    "\n",
    "g = index.get_networkx_graph()\n",
    "net = Network(notebook=True, cdn_resources=\"in_line\", directed=True)\n",
    "net.from_nx(g)\n",
    "net.show(\"example111.html\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>BernoulliNB is a Naive Bayes classifier for data that is distributed according to multivariate Bernoulli distributions. This means that there may be multiple features, but each one is assumed to be a binary-valued (Bernoulli, boolean) variable.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query_engine = index.as_query_engine(\n",
    "    include_text=True,\n",
    "    response_mode=\"tree_summarize\",\n",
    "    embedding_mode=\"hybrid\",\n",
    "    similarity_top_k=5,\n",
    ")\n",
    "response = query_engine.query(\n",
    "    \"Tell me more about BernoulliNB\",\n",
    ")\n",
    "display(Markdown(f\"<b>{response}</b>\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 69,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/markdown": [
       "<b>* BernoulliNB is a Naive Bayes classifier that is specifically designed for binary-valued data. This means that each feature in the data is assumed to be a binary variable, taking on values of 0 or 1.\n",
       "* BernoulliNB is a generative model, which means that it assumes that the data is generated from a particular distribution. In the case of BernoulliNB, the data is assumed to be generated from a multivariate Bernoulli distribution.\n",
       "* BernoulliNB is a relatively simple model, which makes it easy to train and interpret. However, this simplicity also means that it may not be as accurate as more complex models for some datasets.\n",
       "* BernoulliNB can be used for a variety of tasks, including classification and regression. However, it is most commonly used for classification tasks.\n",
       "* BernoulliNB is a popular choice for text classification tasks, as text data is often binary-valued (e.g., a word is either present or absent in a document).\n",
       "\n",
       "Here are some additional details about BernoulliNB:\n",
       "\n",
       "* The probability of a data point belonging to a particular class is given by the following equation:\n",
       "\n",
       "```\n",
       "P(y = c | x) = (P(c) * Î (P(x_i | c))) / P(x)\n",
       "```\n",
       "\n",
       "* Where:\n",
       "    * y is the class label\n",
       "    * c is a particular class\n",
       "    * x is the data point\n",
       "    * x_i is the i-th feature of the data point\n",
       "    * P(c) is the prior probability of class c\n",
       "    * P(x_i | c) is the probability of feature x_i given class c\n",
       "    * P(x) is the probability of the data point\n",
       "\n",
       "* The parameters of the BernoulliNB model are estimated using maximum likelihood estimation.\n",
       "\n",
       "* BernoulliNB can be used with both labeled and unlabeled data. However, it is typically more effective when used with labeled data.</b>"
      ],
      "text/plain": [
       "<IPython.core.display.Markdown object>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "query = \"Tell me more about BernoulliNB\"\n",
    "resp = llm.complete(f\"You are a teching assistant, answer the question {query} with the keeping {response} in mind, do not hallucinate. If the {response} is not enough to answer the question, you can use your abilities along with the {response} to answer the question. If you are not aware of the answer, you can answer it with your pretrained knowledge \")\n",
    "display(Markdown(f\"<b>{resp}</b>\"))\n",
    "\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
